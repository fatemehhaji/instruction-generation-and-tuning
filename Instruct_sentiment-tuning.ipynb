{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "\n",
    "twitter_dataset = \"carblacac/twitter-sentiment-analysis\"\n",
    "alpace_dataset = \"tatsu-lab/alpaca\"\n",
    "\n",
    "train_len = 30000\n",
    "test_len = 50\n",
    "\n",
    "dataset = load_dataset(twitter_dataset)\n",
    "\n",
    "train_twitter = pd.DataFrame(dataset['train'][:train_len])\n",
    "test_twitter = pd.DataFrame(dataset['test'][:test_len])\n",
    "train_twitter['feeling'] = train_twitter['feeling'].astype(str)\n",
    "test_twitter['feeling'] = test_twitter['feeling'].astype(str)\n",
    "\n",
    "\n",
    "dataset_original = load_dataset(alpace_dataset)\n",
    "train_alpaca = dataset_original['train'][:train_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fa6ami86 so happy that salman won.  btw the 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@phantompoptart .......oops.... I guess I'm ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@bradleyjp decidedly undecided. Depends on the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Mountgrace lol i know! its so frustrating isn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kathystover Didn't go much of any where - Lif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text feeling\n",
       "0  @fa6ami86 so happy that salman won.  btw the 1...       0\n",
       "1  @phantompoptart .......oops.... I guess I'm ki...       0\n",
       "2  @bradleyjp decidedly undecided. Depends on the...       1\n",
       "3  @Mountgrace lol i know! its so frustrating isn...       1\n",
       "4  @kathystover Didn't go much of any where - Lif...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "      <th>input_1</th>\n",
       "      <th>input_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fa6ami86 so happy that salman won.  btw the 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>Carefully read the following tweet. Assess the...</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@phantompoptart .......oops.... I guess I'm ki...</td>\n",
       "      <td>0</td>\n",
       "      <td>Carefully read the following tweet. Assess the...</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@bradleyjp decidedly undecided. Depends on the...</td>\n",
       "      <td>1</td>\n",
       "      <td>Carefully read the following tweet. Assess the...</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Mountgrace lol i know! its so frustrating isn...</td>\n",
       "      <td>1</td>\n",
       "      <td>Carefully read the following tweet. Assess the...</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@kathystover Didn't go much of any where - Lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>Carefully read the following tweet. Assess the...</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text feeling  \\\n",
       "0  @fa6ami86 so happy that salman won.  btw the 1...       0   \n",
       "1  @phantompoptart .......oops.... I guess I'm ki...       0   \n",
       "2  @bradleyjp decidedly undecided. Depends on the...       1   \n",
       "3  @Mountgrace lol i know! its so frustrating isn...       1   \n",
       "4  @kathystover Didn't go much of any where - Lif...       1   \n",
       "\n",
       "                                             input_1  \\\n",
       "0  Carefully read the following tweet. Assess the...   \n",
       "1  Carefully read the following tweet. Assess the...   \n",
       "2  Carefully read the following tweet. Assess the...   \n",
       "3  Carefully read the following tweet. Assess the...   \n",
       "4  Carefully read the following tweet. Assess the...   \n",
       "\n",
       "                                             input_2  \n",
       "0  Analyze the mood conveyed in the tweet below, ...  \n",
       "1  Analyze the mood conveyed in the tweet below, ...  \n",
       "2  Analyze the mood conveyed in the tweet below, ...  \n",
       "3  Analyze the mood conveyed in the tweet below, ...  \n",
       "4  Analyze the mood conveyed in the tweet below, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions = {\n",
    "    'instruction_1': \"Carefully read the following tweet. Assess the overall sentiment expressed by considering the choice of words and the tone. Label the tweet with '1' if the overall sentiment is positive, reflecting happiness or satisfaction, and '0' if it reflects a negative sentiment such as sadness or anger.\",\n",
    "    'instruction_2': \"Analyze the mood conveyed in the tweet below, taking into account the language, emojis (if any), and contextual cues. Label the mood as '1' if the tweet communicates a positive, cheerful, or optimistic tone, and '0' if it conveys negativity, pessimism, or discontent.\"\n",
    "}\n",
    "\n",
    "def format_instruction(row, instruction):\n",
    "    # Adding the instruction before the tweet and appending the sentiment analysis result\n",
    "    return f\"{instruction}\\n\\n### Tweet: '{row['text']}'\\n\\n### Sentiment: {row['feeling']}\"\n",
    "\n",
    "# Applying the function to the DataFrame\n",
    "train_twitter['input_1'] = train_twitter.apply(lambda x: format_instruction(x, instructions['instruction_1']), axis=1)\n",
    "train_twitter['input_2'] = train_twitter.apply(lambda x: format_instruction(x, instructions['instruction_2']), axis=1)\n",
    "\n",
    "train_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the mood conveyed in the tweet below, taking into account the language, emojis (if any), and contextual cues. Label the mood as '1' if the tweet communicates a positive, cheerful, or optimistic tone, and '0' if it conveys negativity, pessimism, or discontent.\n",
      "\n",
      "### Tweet: '@fa6ami86 so happy that salman won.  btw the 14sec clip is truely a teaser'\n",
      "\n",
      "### Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "print(train_twitter.iloc[0]['input_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'response'],\n",
      "    num_rows: 60000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_dataset = Dataset.from_pandas(train_twitter[[\"input_2\", \"feeling\"]])\n",
    "hf_dataset = hf_dataset.rename_column(\"input_2\", \"input\").rename_column(\"feeling\", \"response\")\n",
    "\n",
    "if isinstance(train_alpaca, dict):\n",
    "    train_alpaca = Dataset.from_dict({\n",
    "        'input': train_alpaca['text'],\n",
    "        'response': train_alpaca['output']  # Assuming 'response' is the column you want to keep\n",
    "    })  # Directly rename while converting and keep additional column\n",
    "\n",
    "combined_dataset = concatenate_datasets([hf_dataset, train_alpaca]).shuffle(seed=42)\n",
    "print(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'mistralai/Mistral-7B-v0.1'\n",
    "new_model = 'models/Mistral-7B-instruct-sentiment-tuned'\n",
    "output_dir = 'results/Mistral-7B-instruct-finetuned'\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"all\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/storage/fatemeh/organized_projects/NLP_hw3/.instruct/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 60000/60000 [00:04<00:00, 12503.49 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=combined_dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"input\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:55:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.913400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.784600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.749700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.7658797587076823, metrics={'train_runtime': 6913.2369, 'train_samples_per_second': 8.679, 'train_steps_per_second': 0.271, 'total_flos': 3.04636049690198e+17, 'train_loss': 0.7658797587076823, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/Mistral-7B-instruct-sentiment-tuned/tokenizer_config.json',\n",
       " 'models/Mistral-7B-instruct-sentiment-tuned/special_tokens_map.json',\n",
       " 'models/Mistral-7B-instruct-sentiment-tuned/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "new_model = 'models/Mistral-7B-instruct-sentiment-tuned'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(new_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Analyze the mood conveyed in the tweet below, taking into account the language, emojis (if any), and contextual cues. Label the mood as '1' if the tweet communicates a positive, cheerful, or optimistic tone, and '0' if it conveys negativity, pessimism, or discontent.\\n\\n### Tweet: '@justineville ...yeahhh. ) i'm 39 tweets from 1,600!'\\n\\n### Sentiment:\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=new_model, tokenizer=new_model)\n",
    "result = pipe(prompt, max_new_tokens=2)\n",
    "\n",
    "print(result[0]['generated_text'].split('\\n\\n### Sentiment: ')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
