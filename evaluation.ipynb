{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import math\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_models = ['mistralai/Mistral-7B-Instruct-v0.2',\n",
    "                    '/workspace/storage/fatemeh/organized_projects/NLP_hw3/models/Mistral-7B-instruct-sentiment-tuned', \n",
    "                    '/workspace/storage/fatemeh/organized_projects/NLP_hw3/models/Mistral-7B-sentiment-tuned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/storage/fatemeh/organized_projects/NLP_hw3/.instruct/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for carblacac/twitter-sentiment-analysis contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/carblacac/twitter-sentiment-analysis\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "      <th>input_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@justineville ...yeahhh. ) i'm 39 tweets from ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ApplesnFeathers aww. Poor baby! On your only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joeymcintyre With my refunded $225 (Australia...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's fine. Today sucks just because me those t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im just chilling on psp and stuff, but sitting...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text feeling  \\\n",
       "0  @justineville ...yeahhh. ) i'm 39 tweets from ...       1   \n",
       "1  @ApplesnFeathers aww. Poor baby! On your only ...       0   \n",
       "2  @joeymcintyre With my refunded $225 (Australia...       0   \n",
       "3  It's fine. Today sucks just because me those t...       0   \n",
       "4  Im just chilling on psp and stuff, but sitting...       0   \n",
       "\n",
       "                                             input_2  \n",
       "0  Analyze the mood conveyed in the tweet below, ...  \n",
       "1  Analyze the mood conveyed in the tweet below, ...  \n",
       "2  Analyze the mood conveyed in the tweet below, ...  \n",
       "3  Analyze the mood conveyed in the tweet below, ...  \n",
       "4  Analyze the mood conveyed in the tweet below, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "\n",
    "twitter_dataset = \"carblacac/twitter-sentiment-analysis\"\n",
    "\n",
    "train_len = 1000\n",
    "test_len = 50\n",
    "\n",
    "dataset = load_dataset(twitter_dataset)\n",
    "\n",
    "test_twitter = pd.DataFrame(dataset['test'][:test_len])\n",
    "test_twitter['feeling'] = test_twitter['feeling'].astype(str)\n",
    "\n",
    "instructions = {\n",
    "    'instruction_1': \"Carefully read the following tweet. Assess the overall sentiment expressed by considering the choice of words and the tone. Label the tweet with '1' if the overall sentiment is positive, reflecting happiness or satisfaction, and '0' if it reflects a negative sentiment such as sadness or anger.\",\n",
    "    'instruction_2': \"Analyze the mood conveyed in the tweet below, taking into account the language, emojis (if any), and contextual cues. Label the mood as '1' if the tweet communicates a positive, cheerful, or optimistic tone, and '0' if it conveys negativity, pessimism, or discontent.\"\n",
    "}\n",
    "\n",
    "def format_instruction(row, instruction):\n",
    "    return f\"{instruction}\\n\\n### Tweet: '{row['text']}'\\n\\n### Sentiment:\"\n",
    "\n",
    "test_twitter['input_2'] = test_twitter.apply(lambda x: format_instruction(x, instructions['instruction_2']), axis=1)\n",
    "\n",
    "test_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyze the mood conveyed in the tweet below, taking into account the language, emojis (if any), and contextual cues. Label the mood as '1' if the tweet communicates a positive, cheerful, or optimistic tone, and '0' if it conveys negativity, pessimism, or discontent.\n",
      "\n",
      "### Tweet: '@justineville ...yeahhh. ) i'm 39 tweets from 1,600!'\n",
      "\n",
      "### Sentiment:\n"
     ]
    }
   ],
   "source": [
    "print(test_twitter.iloc[0]['input_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# hf_dataset = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, test_dataset):\n",
    "    logging.set_verbosity_error()\n",
    "\n",
    "    # pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0,\n",
    "        max_new_tokens=2,\n",
    "        # top_k=top_k,  \n",
    "        # num_beams=num_beams,  \n",
    "        # temperature=temperature,\n",
    "        # do_sample=True\n",
    "    )\n",
    "\n",
    "    batch_size = 5\n",
    "\n",
    "    num_examples = len(test_dataset)\n",
    "    total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    generated_output = []\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['input_2'] for example in batch]\n",
    "\n",
    "        # Generate text for the batch\n",
    "        results = pipe(prompts, max_new_tokens=2)\n",
    "        \n",
    "        for result in results:\n",
    "            generated_text = result[0]['generated_text'].split('\\n\\n### Sentiment: ')[1]\n",
    "            generated_output.append(generated_text)\n",
    "\n",
    "            # Uncomment the following lines if you want to print the prompts and generated text\n",
    "            # prompt = prompts[results.index(result)]\n",
    "            # print(f\"Prompt: {prompt}\")\n",
    "            # print(f\"Generated Text: {generated_text}\")\n",
    "            # print(\"------\")\n",
    "    \n",
    "    # return [output.split(\"### Response:\\n\")[1].split(\"\\n\\n### Instruction:\")[0].strip() if \"### Response:\\n\" in output else '' for output in generated_output]\n",
    "    return generated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL mistralai/Mistral-7B-Instruct-v0.2 START GENERATING.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "for each_model in finetuned_models:\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(each_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(each_model)\n",
    "\n",
    "    print(f'MODEL {each_model} START GENERATING.')\n",
    "    test_twitter_hf = Dataset.from_pandas(test_twitter)\n",
    "\n",
    "    generated_responses = generate_response(model, tokenizer, test_dataset=test_twitter_hf)\n",
    "\n",
    "    with open(f'results/{each_model.split(\"/\")[-1]}.json', 'w+') as f:\n",
    "        json.dump(generated_responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['/workspace/storage/fatemeh/organized_projects/NLP_hw3/results/Mistral-7B-instruct-sentiment-tuned.json',\n",
    "           '/workspace/storage/fatemeh/organized_projects/NLP_hw3/results/Mistral-7B-sentiment-tuned.json',\n",
    "           '/workspace/storage/fatemeh/organized_projects/NLP_hw3/results/Mistral-7B-Instruct-v0.2.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "      <th>input_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@justineville ...yeahhh. ) i'm 39 tweets from ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ApplesnFeathers aww. Poor baby! On your only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joeymcintyre With my refunded $225 (Australia...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's fine. Today sucks just because me those t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im just chilling on psp and stuff, but sitting...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text feeling  \\\n",
       "0  @justineville ...yeahhh. ) i'm 39 tweets from ...       1   \n",
       "1  @ApplesnFeathers aww. Poor baby! On your only ...       0   \n",
       "2  @joeymcintyre With my refunded $225 (Australia...       0   \n",
       "3  It's fine. Today sucks just because me those t...       0   \n",
       "4  Im just chilling on psp and stuff, but sitting...       0   \n",
       "\n",
       "                                             input_2  \n",
       "0  Analyze the mood conveyed in the tweet below, ...  \n",
       "1  Analyze the mood conveyed in the tweet below, ...  \n",
       "2  Analyze the mood conveyed in the tweet below, ...  \n",
       "3  Analyze the mood conveyed in the tweet below, ...  \n",
       "4  Analyze the mood conveyed in the tweet below, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>feeling</th>\n",
       "      <th>input_2</th>\n",
       "      <th>Mistral-7B-instruct-sentiment-tuned</th>\n",
       "      <th>Mistral-7B-sentiment-tuned</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@justineville ...yeahhh. ) i'm 39 tweets from ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@ApplesnFeathers aww. Poor baby! On your only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@joeymcintyre With my refunded $225 (Australia...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's fine. Today sucks just because me those t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im just chilling on psp and stuff, but sitting...</td>\n",
       "      <td>0</td>\n",
       "      <td>Analyze the mood conveyed in the tweet below, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text feeling  \\\n",
       "0  @justineville ...yeahhh. ) i'm 39 tweets from ...       1   \n",
       "1  @ApplesnFeathers aww. Poor baby! On your only ...       0   \n",
       "2  @joeymcintyre With my refunded $225 (Australia...       0   \n",
       "3  It's fine. Today sucks just because me those t...       0   \n",
       "4  Im just chilling on psp and stuff, but sitting...       0   \n",
       "\n",
       "                                             input_2  \\\n",
       "0  Analyze the mood conveyed in the tweet below, ...   \n",
       "1  Analyze the mood conveyed in the tweet below, ...   \n",
       "2  Analyze the mood conveyed in the tweet below, ...   \n",
       "3  Analyze the mood conveyed in the tweet below, ...   \n",
       "4  Analyze the mood conveyed in the tweet below, ...   \n",
       "\n",
       "  Mistral-7B-instruct-sentiment-tuned Mistral-7B-sentiment-tuned  \\\n",
       "0                                   0                          0   \n",
       "1                                   0                          0   \n",
       "2                                   0                          0   \n",
       "3                                   0                          0   \n",
       "4                                   0                          0   \n",
       "\n",
       "  Mistral-7B-Instruct-v0.2  \n",
       "0                        1  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_results_to_dataframe(filepath, dataframe):\n",
    "    column_name = filepath.split('/')[-1].replace('.json', '')\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Add data as a column to the dataframe\n",
    "    dataframe[column_name] = data\n",
    "\n",
    "# Process each result file\n",
    "for result_path in results:\n",
    "    add_results_to_dataframe(result_path, test_twitter)\n",
    "\n",
    "test_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Mistral-7B-instruct-sentiment-tuned:\n",
      "Precision: 0.9048\n",
      "Recall: 0.8261\n",
      "F1 Score: 0.8636\n",
      "Accuracy: 0.8800\n",
      "\n",
      "Metrics for Mistral-7B-sentiment-tuned:\n",
      "Precision: 0.9048\n",
      "Recall: 0.8261\n",
      "F1 Score: 0.8636\n",
      "Accuracy: 0.8800\n",
      "\n",
      "Metrics for Mistral-7B-Instruct-v0.2:\n",
      "Precision: 0.8000\n",
      "Recall: 0.8696\n",
      "F1 Score: 0.8333\n",
      "Accuracy: 0.8400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def calculate_metrics(true_values, predictions):\n",
    "    true_values = [int(x) for x in true_values]\n",
    "    predictions = [int(x) for x in predictions]\n",
    "\n",
    "    precision = precision_score(true_values, predictions)\n",
    "    recall = recall_score(true_values, predictions)\n",
    "    f1 = f1_score(true_values, predictions)\n",
    "    accuracy = accuracy_score(true_values, predictions)\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "result_columns = [\n",
    "    'Mistral-7B-instruct-sentiment-tuned',\n",
    "    'Mistral-7B-sentiment-tuned',\n",
    "    'Mistral-7B-Instruct-v0.2'\n",
    "]\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for column in result_columns:\n",
    "    p, r, f1, acc = calculate_metrics(test_twitter['feeling'], test_twitter[column])\n",
    "    metrics[column] = {'Precision': p, 'Recall': r, 'F1 Score': f1, 'Accuracy': acc}\n",
    "\n",
    "for model, scores in metrics.items():\n",
    "    print(f\"Metrics for {model}:\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for Alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpace_dataset = \"tatsu-lab/alpaca\"\n",
    "\n",
    "dataset_original = load_dataset(alpace_dataset)\n",
    "train_alpaca = dataset_original['train'][:train_len]\n",
    "test_alpaca = dataset_original['train'][test_len:test_len+25]\n",
    "test_alpaca_hf = Dataset.from_pandas(pd.DataFrame(test_alpaca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, test_dataset):\n",
    "    logging.set_verbosity_error()\n",
    "\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0,\n",
    "        # top_k=top_k,  \n",
    "        # num_beams=num_beams,  \n",
    "        # temperature=temperature  \n",
    "    )\n",
    "\n",
    "    batch_size = 10\n",
    "\n",
    "    num_examples = len(test_dataset)\n",
    "    total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    generated_output = []\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "\n",
    "        # Generate text for the batch\n",
    "        results = pipe(prompts, max_new_tokens=128)\n",
    "        \n",
    "        for result in results:\n",
    "            generated_text = result[0]['generated_text']\n",
    "            generated_output.append(generated_text)\n",
    "\n",
    "            # Uncomment the following lines if you want to print the prompts and generated text\n",
    "            prompt = prompts[results.index(result)]\n",
    "            # print(f\"Prompt: {prompt}\")\n",
    "            # print(f\"Generated Text: {generated_text}\")\n",
    "            # print(\"------\")\n",
    "    \n",
    "    return [output.split(\"### Response:\\n\")[1].split(\"\\n\\n### Instruction:\")[0].strip() if \"### Response:\\n\" in output else '' for output in generated_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model_gpt2 = GPT2LMHeadModel.from_pretrained(model_name).eval()\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    if len(text) == 0:\n",
    "        print(f'THIS {text} RETURN ZERO')\n",
    "        return 0\n",
    "\n",
    "    tokenize_input = tokenizer_gpt2.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        loss = model_gpt2(tokenize_input, labels=tokenize_input)[0]\n",
    "\n",
    "    if not math.isnan(torch.exp(loss).item()):\n",
    "        return torch.exp(loss).item()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = [reference.split()]\n",
    "    candidate_tokens = candidate.split()\n",
    "    smoothie = SmoothingFunction().method1  # You can experiment with different smoothing methods\n",
    "    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothie)\n",
    "\n",
    "def calculate_rouge_l(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    return scorer.score(reference, candidate)['rougeL'].fmeasure\n",
    "\n",
    "def calculate_bert_score(reference, candidate):\n",
    "    *_, bert_scores = score([candidate], [reference], lang='en', return_hash=False)\n",
    "    return bert_scores.mean().item()\n",
    "\n",
    "def evaluate_text_quality(reference, candidate):\n",
    "    return {\n",
    "        'Perplexity': calculate_perplexity(candidate),\n",
    "        'BLEU': calculate_bleu(reference, candidate),\n",
    "        'ROUGE-L': calculate_rouge_l(reference, candidate),\n",
    "        'BERTScore': calculate_bert_score(reference, candidate)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# reference_text = \"This is a sample reference text.\"\n",
    "# generated_text = \"This is a sample generated text.\"\n",
    "# evaluation_results = evaluate_text_quality(reference_text, generated_text)\n",
    "# print(evaluation_results)\n",
    "\n",
    "\n",
    "def calculate_scores(test_dataset, generated_responses):\n",
    "    \"\"\"\n",
    "        Return the scores based on some generated text and the ground truth\n",
    "    \"\"\"\n",
    "    scores = {'Perplexity': 0, 'BLEU': 0, 'ROUGE-L': 0, 'BERTScore': 0}\n",
    "\n",
    "    num_samples = len(test_dataset)\n",
    "\n",
    "    for i, test_data in tqdm(enumerate(test_dataset)):\n",
    "        evaluation_results = evaluate_text_quality(test_data['output'], generated_responses[i])\n",
    "        for key in scores:\n",
    "            scores[key] += evaluation_results[key]\n",
    "\n",
    "    # Average the scores\n",
    "    for key in scores:\n",
    "        scores[key] /= num_samples\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "for each_model in finetuned_models:\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(each_model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(each_model)\n",
    "\n",
    "    print(f'MODEL {each_model} START GENERATING.')\n",
    "    generated_responses = generate_response(model, tokenizer, test_dataset=test_alpaca_hf)\n",
    "\n",
    "    with open(f'results/{each_model.split(\"/\")[-1]}_alpaca.json', 'w+') as f:\n",
    "        json.dump(generated_responses, f)\n",
    "    print(f'MODEL {each_model} START CALCULATING SCORES.')\n",
    "    scores_model = calculate_scores(test_dataset=test_alpaca_hf, generated_responses=generated_responses)\n",
    "\n",
    "    with open(f'results/{each_model.split(\"/\")[-1]}_alpaca_score.json', 'w+') as f:\n",
    "        json.dump(scores_model, f)\n",
    "    print(scores_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Out-of-Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.00it/s]\n",
      "Generating text: 100%|██████████| 10/10 [00:32<00:00,  3.29s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n",
      "Generating text: 100%|██████████| 10/10 [00:39<00:00,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to responses.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Model paths\n",
    "model_sentiment_tuned_path = '/workspace/storage/fatemeh/organized_projects/NLP_hw3/models/Mistral-7B-sentiment-tuned'\n",
    "model_instruct_sentiment_path = '/workspace/storage/fatemeh/organized_projects/NLP_hw3/models/Mistral-7B-instruct-sentiment-tuned'\n",
    "\n",
    "# Create out-of-sample instructions with input examples\n",
    "instructions = [\n",
    "    \"### Instruction:\\nSummarize the following text:\\n\\n### Input:\\nThe history of AI dates back to ancient myths and legends. Modern AI started with the advent of computers.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nTranslate the following sentence into French:\\n\\n### Input:\\nHow are you today?\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nExplain the main idea of this article:\\n\\n### Input:\\nA team of researchers discovered a new way to synthesize clean energy.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nWrite a short poem about the seasons based on the following prompt:\\n\\n### Input:\\nThe changing seasons bring joy and wonder.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nRewrite this paragraph in a more formal tone:\\n\\n### Input:\\nI think the project needs some improvements, but it's a great start.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nExplain this technical concept in simple terms:\\n\\n### Input:\\nQuantum computing harnesses quantum mechanics to perform computations.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nProvide a summary of the key points from this document:\\n\\n### Input:\\nThe company's annual report highlighted increased revenue and market growth.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nPropose a new title for the following article:\\n\\n### Input:\\nThe proposal suggests expanding the product line into new markets.\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nSolve this easy math problem:\\n\\n### Input:\\nWhat is 12 plus 8?\\n\\n### Response:\",\n",
    "    \"### Instruction:\\nIdentify possible corrections for this grammar issue:\\n\\n### Input:\\nI has a apple.\\n\\n### Response:\"\n",
    "]\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_dict({'text': instructions})\n",
    "\n",
    "def generate_response(model_path, test_dataset):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    \n",
    "    batch_size = 1\n",
    "    num_examples = len(test_dataset)\n",
    "    total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    generated_output = []\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'] for example in batch]\n",
    "\n",
    "        results = pipe(prompts, max_new_tokens=128)\n",
    "        \n",
    "        for result in results:\n",
    "            generated_text = result[0]['generated_text']\n",
    "            generated_output.append(generated_text)\n",
    "\n",
    "    return generated_output\n",
    "\n",
    "responses_instruct_sentiment = generate_response(model_instruct_sentiment_path, test_dataset)\n",
    "responses_sentiment_tuned = generate_response(model_sentiment_tuned_path, test_dataset)\n",
    "\n",
    "results_json = []\n",
    "for i, instruction in enumerate(instructions):\n",
    "    results_json.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"response_instruct_sentiment\": responses_instruct_sentiment[i],\n",
    "        \"response_sentiment_tuned\": responses_sentiment_tuned[i]\n",
    "    })\n",
    "\n",
    "with open('/workspace/storage/fatemeh/organized_projects/NLP_hw3/results/responses.json', 'w+') as f:\n",
    "    json.dump(results_json, f, indent=4)\n",
    "\n",
    "print(\"Results saved to responses.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
